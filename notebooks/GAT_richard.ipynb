{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOSqMqd8wQkf"
   },
   "source": [
    "# installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Nzrbl8xs5gm",
    "outputId": "dc507903-6661-4bbf-eb13-279257540eb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (3.11.11)\n",
      "Requirement already satisfied: fsspec in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (1.24.3)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: pyparsing in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from torch_geometric) (4.67.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from requests->torch_geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from requests->torch_geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RpYbrQGs8wl",
    "outputId": "c4acfd06-36b6-4a7a-9316-60afee08c94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from umap-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from umap-learn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from umap-learn) (1.5.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from umap-learn) (0.60.0)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from umap-learn) (4.67.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manoj/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n"
     ]
    }
   ],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcvep5adwU1b"
   },
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jl04snr1tAYz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, SAGEConv, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import logging\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import umap\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfWx-vLdwYcs"
   },
   "source": [
    "# Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CaV2OSbptEjw"
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='training.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jgs77h_swdFL"
   },
   "source": [
    "# Predicting new collaborations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sOtgXaM2tInW"
   },
   "outputs": [],
   "source": [
    "def predict_new_collaborations(model, data, nodes_df, node_index_map, num_predictions=10):\n",
    "    \"\"\"Predict potential new collaborations.\"\"\"\n",
    "    device = data.x.device\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_nodes = data.num_nodes\n",
    "        # generate potential collaborations\n",
    "        unseen_pairs = generate_negative_edges(num_nodes, data.edge_index, 1000).to(device)\n",
    "\n",
    "        embeddings = model(data.x, data.edge_index)\n",
    "        predictions = model.predict_edges(embeddings, unseen_pairs)\n",
    "\n",
    "        top_indices = torch.argsort(predictions, descending=True)[:num_predictions]\n",
    "        top_pairs = unseen_pairs[top_indices]\n",
    "        top_scores = predictions[top_indices]\n",
    "\n",
    "        reverse_mapping = {v: k for k, v in node_index_map.items()}\n",
    "        results = []\n",
    "\n",
    "        for (i, j), score in zip(top_pairs.tolist(), top_scores.tolist()):\n",
    "            artist1_id = reverse_mapping[i]\n",
    "            artist2_id = reverse_mapping[j]\n",
    "            artist1_name = nodes_df[nodes_df['spotify_id'] == artist1_id]['name'].iloc[0]\n",
    "            artist2_name = nodes_df[nodes_df['spotify_id'] == artist2_id]['name'].iloc[0]\n",
    "\n",
    "            results.append({\n",
    "                'artist1': artist1_name,\n",
    "                'artist2': artist2_name,\n",
    "                'score': score\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rLUMlTkwh3_"
   },
   "source": [
    "# visualizing TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T1FY99umtMeB"
   },
   "outputs": [],
   "source": [
    "def visualize_tsne_embeddings(embeddings, nodes_df, genre_columns, filename=\"tsne_embeddings.png\"):\n",
    "    \"\"\"Visualize t-SNE embeddings of the nodes.\"\"\"\n",
    "\n",
    "    # convert embeddings to CPU if necessary\n",
    "    if isinstance(embeddings, torch.Tensor):\n",
    "        embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "    # reduce embeddings to 2D using t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # select genre-based coloring\n",
    "    genre_labels = nodes_df[genre_columns].idxmax(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], hue=genre_labels, palette=\"tab10\", alpha=0.7)\n",
    "\n",
    "    plt.title(\"t-SNE Visualization of Node Embeddings\")\n",
    "    plt.xlabel(\"t-SNE Dim 1\")\n",
    "    plt.ylabel(\"t-SNE Dim 2\")\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ETP0Ovzwpa7"
   },
   "source": [
    "# plotting degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G60bbvvatQcI"
   },
   "outputs": [],
   "source": [
    "def plot_degree_distribution(edge_index, filename=\"degree_distribution.png\"):\n",
    "    \"\"\"Plot the degree distribution of the graph.\"\"\"\n",
    "\n",
    "    # compute node degrees\n",
    "    degrees = torch.bincount(edge_index.flatten()).cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(degrees, bins=50, log=True, alpha=0.7, color='b', edgecolor='black')\n",
    "    plt.xlabel(\"Degree\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.title(\"Node Degree Distribution\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pnLnlWJwuJd"
   },
   "source": [
    "# plotting UMAP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WluyQMNdtUG4"
   },
   "outputs": [],
   "source": [
    "def visualize_umap_embeddings(embeddings, nodes_df, genre_columns, filename=\"umap_embeddings.png\"):\n",
    "    \"\"\"Visualize UMAP embeddings of the nodes.\"\"\"\n",
    "\n",
    "    # convert embeddings to CPU if necessary\n",
    "    if isinstance(embeddings, torch.Tensor):\n",
    "        embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "    # reduce embeddings to 2D using UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # select genre-based coloring\n",
    "    genre_labels = nodes_df[genre_columns].idxmax(axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], hue=genre_labels, palette=\"tab10\", alpha=0.7)\n",
    "\n",
    "    plt.title(\"UMAP Visualization of Node Embeddings\")\n",
    "    plt.xlabel(\"UMAP Dim 1\")\n",
    "    plt.ylabel(\"UMAP Dim 2\")\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2QDkoGaw1it"
   },
   "source": [
    "# Generating negatives edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BjK9XE2LtXpm"
   },
   "outputs": [],
   "source": [
    "def generate_negative_edges(num_nodes, edge_index, num_samples):\n",
    "    \"\"\"Generate negative edges (non-collaborations).\"\"\"\n",
    "    existing_edges = set(map(tuple, edge_index.t().tolist()))\n",
    "    negative_edges = set()\n",
    "\n",
    "    while len(negative_edges) < num_samples:\n",
    "        i, j = random.randint(0, num_nodes - 1), random.randint(0, num_nodes - 1)\n",
    "        if i != j and (i, j) not in existing_edges and (j, i) not in existing_edges:\n",
    "            negative_edges.add((i, j))\n",
    "\n",
    "    return torch.tensor(list(negative_edges), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_SzZTcHw6Rj"
   },
   "source": [
    "# analyzing false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qeQNNXcNtalI"
   },
   "outputs": [],
   "source": [
    "def analyze_false_positives(predictions, nodes_df, genre_columns):\n",
    "    \"\"\"Analyze false positives to understand model behavior.\"\"\"\n",
    "    analysis = []\n",
    "    for pred in predictions:\n",
    "        artist1 = pred['artist1']\n",
    "        artist2 = pred['artist2']\n",
    "\n",
    "        # debug print to check if artists are found\n",
    "        artist1_data = nodes_df[nodes_df['name'] == artist1]\n",
    "        artist2_data = nodes_df[nodes_df['name'] == artist2]\n",
    "\n",
    "        if artist1_data.empty or artist2_data.empty:\n",
    "            print(f\"Warning: Could not find data for {artist1 if artist1_data.empty else artist2}\")\n",
    "            continue\n",
    "\n",
    "        # get genres with explicit boolean conversion\n",
    "        artist1_genres = artist1_data[genre_columns].iloc[0] > 0\n",
    "        artist2_genres = artist2_data[genre_columns].iloc[0] > 0\n",
    "\n",
    "        # get genre names where both artists have True\n",
    "        shared_genres = [genre for genre, (has1, has2) in\n",
    "                        zip(genre_columns, zip(artist1_genres, artist2_genres))\n",
    "                        if has1 and has2]\n",
    "\n",
    "        # add individual genres for context\n",
    "        artist1_genre_list = [genre for genre, has_genre in zip(genre_columns, artist1_genres) if has_genre]\n",
    "        artist2_genre_list = [genre for genre, has_genre in zip(genre_columns, artist2_genres) if has_genre]\n",
    "\n",
    "        analysis.append({\n",
    "            'artist1': artist1,\n",
    "            'artist2': artist2,\n",
    "            'score': pred['score'],\n",
    "            'shared_genres': shared_genres,\n",
    "            'artist1_genres': artist1_genre_list,\n",
    "            'artist2_genres': artist2_genre_list\n",
    "        })\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmXYyJK9xBNV"
   },
   "source": [
    "# plotting training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vBWJSRW4teAo"
   },
   "outputs": [],
   "source": [
    "def plot_training_metrics(loss_history, val_loss_history, roc_auc_history):\n",
    "    \"\"\"plot training metrics over time.\"\"\"\n",
    "\n",
    "    # use the length of loss_history for all plots\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    # Create x-axis data for ROC-AUC plot, matching the update frequency\n",
    "    roc_auc_epochs = range(10, len(loss_history) + 1, 10)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, loss_history, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_history, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot ROC-AUC using the correct x-axis data\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(roc_auc_epochs, roc_auc_history, 'g-', label='ROC-AUC')  # Updated line\n",
    "    plt.title('ROC-AUC Score Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROC-AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_metrics.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3e8PTvKxbWK"
   },
   "source": [
    "# plotting genre performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PoN0hKhdtkuY"
   },
   "outputs": [],
   "source": [
    "def plot_genre_performance(genre_performance):\n",
    "    \"\"\"Visualize model performance across different genres.\"\"\"\n",
    "    genres = list(genre_performance.keys())\n",
    "    roc_scores = [perf['roc_auc'] for perf in genre_performance.values()]\n",
    "    f1_scores = [perf['f1'] for perf in genre_performance.values()]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = range(len(genres))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.bar([i - width/2 for i in x], roc_scores, width, label='ROC-AUC')\n",
    "    plt.bar([i + width/2 for i in x], f1_scores, width, label='F1-Score')\n",
    "\n",
    "    plt.xlabel('Genres')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance by Genre')\n",
    "    plt.xticks(x, genres, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('genre_performance.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbQ1c1tgxkNF"
   },
   "source": [
    "# Validate and analyze input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7_-6PuWNtoYj"
   },
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"Validate and analyze input data.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_dataframe(df: pd.DataFrame, required_columns: List[str]) -> bool:\n",
    "        \"\"\"validate DataFrame structure and content.\"\"\"\n",
    "        # Check required columns\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            logging.error(f\"Missing required columns: {missing_cols}\")\n",
    "            return False\n",
    "\n",
    "        # Check for null values\n",
    "        null_counts = df.isnull().sum()\n",
    "        if null_counts.any():\n",
    "            logging.warning(f\"Null values found:\\n{null_counts[null_counts > 0]}\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data_report(nodes_df: pd.DataFrame, edges_df: pd.DataFrame, genre_columns: List[str]) -> Dict:\n",
    "      \"\"\"Generate comprehensive data statistics report.\"\"\"\n",
    "\n",
    "      # Convert 'followers' and 'popularity' columns to numeric before calculating the mean\n",
    "      nodes_df['followers'] = pd.to_numeric(nodes_df['followers'], errors='coerce')\n",
    "      nodes_df['popularity'] = pd.to_numeric(nodes_df['popularity'], errors='coerce')\n",
    "\n",
    "      report = {\n",
    "          'nodes_stats': {\n",
    "              'total_nodes': len(nodes_df),\n",
    "              'avg_followers': nodes_df['followers'].mean(),\n",
    "              'avg_popularity': nodes_df['popularity'].mean(),\n",
    "              'genre_distribution': {},\n",
    "              'missing_data': nodes_df.isnull().sum().to_dict()\n",
    "          },\n",
    "          'edges_stats': {\n",
    "              'total_edges': len(edges_df),\n",
    "              'unique_artists': len(set(edges_df['id_0'].unique()) | set(edges_df['id_1'].unique())),\n",
    "              'avg_collaborations_per_artist': len(edges_df) / len(nodes_df)\n",
    "          }\n",
    "      }\n",
    "\n",
    "      # Genre distribution analysis\n",
    "      for genre in genre_columns:\n",
    "          # Convert NumPy int64 to native Python int\n",
    "          report['nodes_stats']['genre_distribution'][genre] = int(nodes_df[genre].sum())\n",
    "\n",
    "      return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOYACCAPxoXM"
   },
   "source": [
    "# Analyzing Genre distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9KeUw-wyttkv"
   },
   "outputs": [],
   "source": [
    "def analyze_genre_distribution(nodes_df: pd.DataFrame, genre_columns: List[str]) -> None:\n",
    "    \"\"\"Visualize genre distribution in the dataset.\"\"\"\n",
    "    genre_counts = {genre: nodes_df[genre].sum() for genre in genre_columns}\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(genre_counts.keys(), genre_counts.values())\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Genre Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('genre_distribution.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYaQuF7Axujy"
   },
   "source": [
    "# GAT MODEL implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1S_iUM4Ftwls"
   },
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=8):\n",
    "        \"\"\"Graph Attention Network for link prediction.\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=0.6)\n",
    "        self.link_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.6),\n",
    "            torch.nn.Linear(hidden_channels, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def predict_edges(self, x, edge_index):\n",
    "        row, col = edge_index.t()\n",
    "        edge_rep = torch.cat([x[row], x[col]], dim=1)\n",
    "        return self.link_predictor(edge_rep).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "D9Lo2wnPt0Mq"
   },
   "outputs": [],
   "source": [
    "# enhanced data loading with validation\n",
    "def load_data(validate: bool = True) -> Tuple:\n",
    "    \"\"\"Load and preprocess data with validation.\"\"\"\n",
    "    # set low_memory=False to handle mixed types warning\n",
    "    nodes_df = pd.read_csv(\"nodes_cleaned.csv\",\n",
    "                          dtype={'spotify_id': str},\n",
    "                          low_memory=False)\n",
    "    edges_df = pd.read_csv(\"edges_cleaned.csv\")\n",
    "\n",
    "    if validate:\n",
    "      print(\"  • Validating data structure...\")\n",
    "      required_columns = ['spotify_id', 'name', 'followers', 'popularity']\n",
    "      if not DataValidator.validate_dataframe(nodes_df, required_columns):\n",
    "          raise ValueError(\"Data validation failed\")\n",
    "\n",
    "    # handle null values before processing\n",
    "    nodes_df['followers'] = pd.to_numeric(nodes_df['followers'], errors='coerce')\n",
    "    nodes_df['popularity'] = pd.to_numeric(nodes_df['popularity'], errors='coerce')\n",
    "\n",
    "    # drop rows with null values in critical columns\n",
    "    critical_columns = ['spotify_id', 'name', 'followers', 'popularity']\n",
    "    nodes_df = nodes_df.dropna(subset=critical_columns)\n",
    "\n",
    "    # rest of the preprocessing remains the same...\n",
    "    nodes_df = nodes_df.drop_duplicates(subset=['spotify_id']).reset_index(drop=True)\n",
    "    nodes_df = nodes_df[nodes_df['unknown'] == 0].reset_index(drop=True)\n",
    "\n",
    "    # drop Unnamed columns if they exist\n",
    "    unnamed_cols = [col for col in nodes_df.columns if 'Unnamed:' in col]\n",
    "    if unnamed_cols:\n",
    "        nodes_df = nodes_df.drop(columns=unnamed_cols)\n",
    "\n",
    "    # genre handling remains the same\n",
    "    for col in ['alternative Indie', 'folk world']:\n",
    "        if col in nodes_df.columns:\n",
    "            nodes_df[col] = nodes_df[col].astype(str)\n",
    "\n",
    "    # split combined columns remains the same\n",
    "    for col_pair, combined_col in [(['alternative', 'Indie'], 'alternative Indie'),\n",
    "                                   (['folk', 'world'], 'folk world')]:\n",
    "        if combined_col in nodes_df.columns:\n",
    "            dummy_df = nodes_df[combined_col].str.get_dummies(sep=' ')\n",
    "            for col in col_pair:\n",
    "                if col not in dummy_df.columns:\n",
    "                    dummy_df[col] = 0\n",
    "            nodes_df[col_pair] = dummy_df[col_pair].fillna(0)\n",
    "            nodes_df = nodes_df.drop(columns=[combined_col])\n",
    "\n",
    "    genre_columns = [\n",
    "        'alternative', 'Indie', 'classical_orchestral', 'electronic',\n",
    "        'folk', 'world', 'jazz', 'hip_hop', 'latin', 'metal', 'pop',\n",
    "        'randb_Soul', 'reggae_dancehall', 'rock', 'soundtrack'\n",
    "    ]\n",
    "\n",
    "    # Generate data report\n",
    "    report = DataValidator.generate_data_report(nodes_df, edges_df, genre_columns)\n",
    "    with open('data_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    # Visualize genre distribution\n",
    "    analyze_genre_distribution(nodes_df, genre_columns)\n",
    "\n",
    "    # Continue with original preprocessing\n",
    "    for col in ['followers', 'popularity']:\n",
    "        # Convert the column to numeric, coercing errors to NaN\n",
    "        nodes_df[col] = pd.to_numeric(nodes_df[col], errors='coerce')\n",
    "        # Fill NaN values with 0\n",
    "        nodes_df[col] = nodes_df[col].fillna(0)  # Change: Fill NaN with 0\n",
    "        # Now normalize\n",
    "        nodes_df[col] = (nodes_df[col] - nodes_df[col].min()) / (nodes_df[col].max() - nodes_df[col].min())\n",
    "\n",
    "    node_index_map = {spotify_id: idx for idx, spotify_id in enumerate(nodes_df['spotify_id'])}\n",
    "\n",
    "    valid_edges = edges_df[\n",
    "        edges_df['id_0'].isin(node_index_map) &\n",
    "        edges_df['id_1'].isin(node_index_map)\n",
    "    ]\n",
    "\n",
    "    edge_index = torch.tensor([\n",
    "        [node_index_map[id_] for id_ in valid_edges['id_0']],\n",
    "        [node_index_map[id_] for id_ in valid_edges['id_1']]\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    node_features_df = nodes_df[['followers', 'popularity'] + genre_columns]\n",
    "    node_features_df = node_features_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    node_features = torch.tensor(node_features_df.values, dtype=torch.float)\n",
    "    # Add this to load_data():\n",
    "    print(f\"Total artists: {len(nodes_df)}\")\n",
    "    print(f\"Artists with genre info: {len(nodes_df[nodes_df[genre_columns].sum(axis=1) > 0])}\")\n",
    "\n",
    "    return nodes_df, edge_index, node_features, node_index_map, genre_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqdF5oF-x8or"
   },
   "source": [
    "# Alternative GNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iMEGTMhRt6E8"
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.link_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.6),\n",
    "            torch.nn.Linear(hidden_channels, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def predict_edges(self, x, edge_index):\n",
    "        row, col = edge_index.t()\n",
    "        edge_rep = torch.cat([x[row], x[col]], dim=1)\n",
    "        return self.link_predictor(edge_rep).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9wnzbpXyCb1"
   },
   "source": [
    "# GraphSage model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MxwGvcTkt-ow"
   },
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.link_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.6),\n",
    "            torch.nn.Linear(hidden_channels, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def predict_edges(self, x, edge_index):\n",
    "        row, col = edge_index.t()\n",
    "        edge_rep = torch.cat([x[row], x[col]], dim=1)\n",
    "        return self.link_predictor(edge_rep).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8FRSHpYyHgQ"
   },
   "source": [
    "# Validate model architecture and analyze complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UXyjdDzQuD63"
   },
   "outputs": [],
   "source": [
    "class ModelValidator:\n",
    "    \"\"\"Validate model architecture and analyze complexity.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def count_parameters(model: torch.nn.Module) -> int:\n",
    "        \"\"\"Count trainable parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_model_complexity(model: torch.nn.Module) -> Dict:\n",
    "        \"\"\"Analyze model complexity and architecture.\"\"\"\n",
    "        return {\n",
    "            'total_params': ModelValidator.count_parameters(model),\n",
    "            'layers': [\n",
    "                {\n",
    "                    'name': name,\n",
    "                    'params': sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "                }\n",
    "                for name, module in model.named_children()\n",
    "            ]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHWh5quryKgz"
   },
   "source": [
    "# Tune model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0G8wfRHsuIuR"
   },
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    \"\"\"Tune model hyperparameters.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def grid_search(model_class, data, param_grid: Dict, n_splits: int = 5) -> Dict:\n",
    "        \"\"\"Perform grid search for hyperparameter tuning.\"\"\"\n",
    "        device = data.x.device  # Get the device from data\n",
    "        best_params = {}\n",
    "        best_score = 0\n",
    "\n",
    "        # Initialize with default values\n",
    "        best_params['hidden_channels'] = param_grid['hidden_channels'][0]\n",
    "        best_params['heads'] = param_grid['heads'][0]\n",
    "\n",
    "        for params in tqdm(HyperparameterTuner._parameter_combinations(param_grid)):\n",
    "            scores = []\n",
    "            kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "            for train_idx, val_idx in kf.split(range(data.num_nodes)):\n",
    "                # Create model and move to correct device\n",
    "                model = model_class(\n",
    "                    in_channels=params['in_channels'],\n",
    "                    hidden_channels=params['hidden_channels'],\n",
    "                    heads=params['heads']\n",
    "                ).to(device)\n",
    "\n",
    "                score = HyperparameterTuner._validate_model(model, data, train_idx, val_idx)\n",
    "                scores.append(score)\n",
    "\n",
    "            avg_score = np.mean(scores)\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_model(model, data, train_idx, val_idx) -> float:\n",
    "        \"\"\"Validate model performance for a specific parameter combination.\"\"\"\n",
    "        device = data.x.device\n",
    "        train_idx = torch.tensor(train_idx, device=device)\n",
    "        val_idx = torch.tensor(val_idx, device=device)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        # Simple forward pass and validation\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(data.x, data.edge_index)\n",
    "            # Simple validation metric (e.g., L2 norm of embeddings)\n",
    "            score = torch.norm(embeddings[val_idx]).item()\n",
    "\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def _parameter_combinations(param_grid: Dict) -> List[Dict]:\n",
    "        \"\"\"Generate all possible parameter combinations.\"\"\"\n",
    "        keys = param_grid.keys()\n",
    "        values = param_grid.values()\n",
    "        for instance in itertools.product(*values):\n",
    "            yield dict(zip(keys, instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PLxfYTSxuYgL"
   },
   "outputs": [],
   "source": [
    "def train_with_validation(model, data, train_edges, val_edges, optimizer, scheduler):\n",
    "    \"\"\"Train the model with validation.\"\"\"\n",
    "    device = next(model.parameters()).device  # Get the device the model is on\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Training\n",
    "    neg_edges = generate_negative_edges(data.num_nodes, data.edge_index, train_edges.size(0))\n",
    "    embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "    pos_pred = model.predict_edges(embeddings, train_edges)\n",
    "    neg_pred = model.predict_edges(embeddings, neg_edges)\n",
    "\n",
    "    pred = torch.cat([pos_pred, neg_pred])\n",
    "    labels = torch.cat([\n",
    "        torch.ones(pos_pred.size(0), device=device),\n",
    "        torch.zeros(neg_pred.size(0), device=device)\n",
    "    ])\n",
    "\n",
    "    loss = F.binary_cross_entropy(pred, labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_neg_edges = generate_negative_edges(data.num_nodes, data.edge_index, val_edges.size(0))\n",
    "        val_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "        val_pos_pred = model.predict_edges(val_embeddings, val_edges)\n",
    "        val_neg_pred = model.predict_edges(val_embeddings, val_neg_edges)\n",
    "\n",
    "        val_pred = torch.cat([val_pos_pred, val_neg_pred])\n",
    "        val_labels = torch.cat([\n",
    "            torch.ones(val_pos_pred.size(0), device=val_pred.device),\n",
    "            torch.zeros(val_neg_pred.size(0), device=val_pred.device)\n",
    "        ])\n",
    "\n",
    "        val_loss = F.binary_cross_entropy(val_pred, val_labels.float())\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    return loss.item(), val_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "uyZ16US-uNIc"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, data, test_edges):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device  # Get the device the model is on\n",
    "\n",
    "    with torch.no_grad():\n",
    "        neg_edges = generate_negative_edges(data.num_nodes, data.edge_index, test_edges.size(0))\n",
    "        embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "        pos_pred = model.predict_edges(embeddings, test_edges).cpu()\n",
    "        neg_pred = model.predict_edges(embeddings, neg_edges).cpu()\n",
    "\n",
    "        pred = torch.cat([pos_pred, neg_pred])\n",
    "        labels = torch.cat([\n",
    "            torch.ones(pos_pred.size(0)),\n",
    "            torch.zeros(neg_pred.size(0))\n",
    "        ])\n",
    "\n",
    "        # Calculate metrics\n",
    "        roc_auc = roc_auc_score(labels, pred)\n",
    "        precision, recall, _ = precision_recall_curve(labels, pred)\n",
    "        avg_precision = average_precision_score(labels, pred)\n",
    "        conf_matrix = confusion_matrix(labels, (pred > 0.5).float())\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot precision-recall curve\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, label=f'AP={avg_precision:.2f}')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('precision_recall_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        return {\n",
    "            'roc_auc': roc_auc,\n",
    "            'average_precision': avg_precision,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "GRwMqXDrujBU"
   },
   "outputs": [],
   "source": [
    "def ensure_device(tensor, device):\n",
    "    \"\"\"Ensure a tensor is on the specified device.\"\"\"\n",
    "    if tensor.device != device:\n",
    "        return tensor.to(device)\n",
    "    return tensor\n",
    "\n",
    "def analyze_genre_specific_performance(model, data, test_edges, nodes_df, genre_columns):\n",
    "    \"\"\"Analyze model performance for specific genres.\"\"\"\n",
    "    model.eval()\n",
    "    device = data.x.device\n",
    "    genre_performance = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "        for genre in genre_columns:\n",
    "            # Get artists in this genre\n",
    "            genre_artists = nodes_df[nodes_df[genre] == 1]['spotify_id'].values\n",
    "            genre_indices = [nodes_df[nodes_df['spotify_id'] == aid].index[0] for aid in genre_artists]\n",
    "            genre_indices_tensor = ensure_device(torch.tensor(genre_indices), device)\n",
    "\n",
    "            # Filter test edges involving these artists\n",
    "            genre_edges = test_edges[\n",
    "                (test_edges[:, 0].unsqueeze(1) == genre_indices_tensor).any(1) &\n",
    "                (test_edges[:, 1].unsqueeze(1) == genre_indices_tensor).any(1)\n",
    "            ]\n",
    "\n",
    "            if len(genre_edges) > 0:\n",
    "                neg_edges = generate_negative_edges(data.num_nodes, data.edge_index, genre_edges.size(0))\n",
    "\n",
    "                pos_pred = model.predict_edges(embeddings, genre_edges).cpu()\n",
    "                neg_pred = model.predict_edges(embeddings, neg_edges).cpu()\n",
    "\n",
    "                pred = torch.cat([pos_pred, neg_pred])\n",
    "                labels = torch.cat([\n",
    "                    torch.ones(pos_pred.size(0)),\n",
    "                    torch.zeros(neg_pred.size(0))\n",
    "                ])\n",
    "\n",
    "                genre_performance[genre] = {\n",
    "                    'roc_auc': roc_auc_score(labels, pred),\n",
    "                    'f1': f1_score(labels, (pred > 0.5).float())\n",
    "                }\n",
    "\n",
    "    return genre_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "rq1JH5LBunBR"
   },
   "outputs": [],
   "source": [
    "def train_with_cross_validation(model_class, data, k_folds=5):\n",
    "    \"\"\"Train model using k-fold cross-validation.\"\"\"\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    all_edges = data.edge_index.t().contiguous()\n",
    "    cv_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(all_edges)))):\n",
    "        logging.info(f\"Starting fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        train_edges = all_edges[train_idx]\n",
    "        val_edges = all_edges[val_idx]\n",
    "\n",
    "        model = model_class(in_channels=data.num_node_features, hidden_channels=64)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "        best_val_score = 0\n",
    "        for epoch in range(200):\n",
    "            loss, val_loss = train_with_validation(model, data, train_edges, val_edges, optimizer, scheduler)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                val_metrics = evaluate_model_performance(model, data, val_edges)\n",
    "                val_score = val_metrics['roc_auc']\n",
    "\n",
    "                if val_score > best_val_score:\n",
    "                    best_val_score = val_score\n",
    "                    torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')\n",
    "\n",
    "                logging.info(f\"Fold {fold + 1}, Epoch {epoch}: Loss = {loss:.4f}, Val Loss = {val_loss:.4f}, Val ROC-AUC = {val_score:.4f}\")\n",
    "\n",
    "        cv_scores.append(best_val_score)\n",
    "        logging.info(f\"Fold {fold + 1} completed. Best validation ROC-AUC: {best_val_score:.4f}\")\n",
    "\n",
    "    return np.mean(cv_scores), np.std(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "15i-9tCPurnh"
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"Save model with proper device handling.\"\"\"\n",
    "    torch.save(model.state_dict(), path)  # Change this line to save only the state_dict\n",
    "def load_model(model, path, device):\n",
    "    \"\"\"Load model with proper device handling.\"\"\"\n",
    "    model.load_state_dict(torch.load(path, map_location=device))  # Load the state_dict and remove weights_only\n",
    "    return model\n",
    "\n",
    "def compare_model_architectures(data, train_edges, val_edges, test_edges):\n",
    "    \"\"\"Compare different GNN architectures.\"\"\"\n",
    "    device = data.x.device\n",
    "    models = {\n",
    "        'GAT': GAT(in_channels=data.num_node_features, hidden_channels=64).to(device),\n",
    "        'GCN': GCN(in_channels=data.num_node_features, hidden_channels=64).to(device),\n",
    "        'GraphSAGE': GraphSAGE(in_channels=data.num_node_features, hidden_channels=64).to(device)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        logging.info(f\"Training {name} model...\")\n",
    "        model = model.to(device)  # Ensure model is on correct device\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "        # Training loop\n",
    "        best_val_score = 0\n",
    "        for epoch in range(200):\n",
    "            loss, val_loss = train_with_validation(model, data, train_edges, val_edges, optimizer, scheduler)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                val_metrics = evaluate_model_performance(model, data, val_edges)\n",
    "                val_score = val_metrics['roc_auc']\n",
    "\n",
    "                if val_score > best_val_score:\n",
    "                    best_val_score = val_score\n",
    "                    save_model(model, f'best_{name}_model.pth')\n",
    "\n",
    "        # Final evaluation\n",
    "        model = load_model(model, f'best_{name}_model.pth', device)\n",
    "        test_metrics = evaluate_model_performance(model, data, test_edges)\n",
    "        results[name] = test_metrics\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mB4V04etv4p6"
   },
   "source": [
    "# Main function without TSNE visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg2qLXbKv9w4",
    "outputId": "4c74beeb-0c2d-4e3a-9e7f-c073a9042970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Project Requirements Tracking ===\n",
      "Using device: cpu\n",
      "\n",
      "✓ Requirement 1.1: Device Setup Complete - Using cpu\n",
      "\n",
      "--- Starting Data Loading and Validation ---\n",
      "  • Validating data structure...\n",
      "Total artists: 120353\n",
      "Artists with genre info: 25576\n",
      "✓ Requirement 1.2: Data Loading and Validation Complete\n",
      "  • Loaded 120353 artists\n",
      "  • Processed 111799 collaborations\n",
      "  • Extracted 15 genre features\n",
      "✓ Requirement 1.3: Data Movement to Device Complete\n",
      "\n",
      "--- Starting Data Splitting ---\n",
      "✓ Requirement 2.1: Data Splitting Complete\n",
      "  • Training edges: 78259\n",
      "  • Validation edges: 16770\n",
      "  • Test edges: 16770\n",
      "\n",
      "--- Starting Hyperparameter Tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:56,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Requirement 2.2: Hyperparameter Tuning Complete\n",
      "  • Best parameters found: {'in_channels': 17, 'hidden_channels': 128, 'heads': 4}\n",
      "\n",
      "--- Starting Model Training ---\n",
      "✓ Requirement 3.1: Model Initialization Complete\n",
      "✓ Requirement 3.2: Optimizer and Scheduler Setup Complete\n",
      "\n",
      "--- Starting Training Loop ---\n",
      "Epoch: 010, Loss: 0.5745, Val Loss: 0.5586, Val ROC-AUC: 0.8711\n",
      "Epoch: 020, Loss: 0.5624, Val Loss: 0.5557, Val ROC-AUC: 0.8601\n",
      "Epoch: 030, Loss: 0.5573, Val Loss: 0.5460, Val ROC-AUC: 0.8533\n",
      "Epoch: 040, Loss: 0.5552, Val Loss: 0.5499, Val ROC-AUC: 0.8496\n",
      "Epoch: 050, Loss: 0.5498, Val Loss: 0.5503, Val ROC-AUC: 0.8500\n",
      "Epoch: 060, Loss: 0.5486, Val Loss: 0.5471, Val ROC-AUC: 0.8520\n",
      "Epoch: 070, Loss: 0.5487, Val Loss: 0.5511, Val ROC-AUC: 0.8494\n",
      "Epoch: 080, Loss: 0.5507, Val Loss: 0.5491, Val ROC-AUC: 0.8523\n",
      "Epoch: 090, Loss: 0.5506, Val Loss: 0.5490, Val ROC-AUC: 0.8515\n",
      "Epoch: 100, Loss: 0.5501, Val Loss: 0.5486, Val ROC-AUC: 0.8517\n",
      "Epoch: 110, Loss: 0.5478, Val Loss: 0.5488, Val ROC-AUC: 0.8493\n",
      "Epoch: 120, Loss: 0.5487, Val Loss: 0.5497, Val ROC-AUC: 0.8483\n",
      "Epoch: 130, Loss: 0.5458, Val Loss: 0.5480, Val ROC-AUC: 0.8503\n",
      "Epoch: 140, Loss: 0.5506, Val Loss: 0.5467, Val ROC-AUC: 0.8519\n",
      "Epoch: 150, Loss: 0.5473, Val Loss: 0.5474, Val ROC-AUC: 0.8498\n",
      "Epoch: 160, Loss: 0.5484, Val Loss: 0.5491, Val ROC-AUC: 0.8509\n",
      "Epoch: 170, Loss: 0.5515, Val Loss: 0.5499, Val ROC-AUC: 0.8505\n",
      "Epoch: 180, Loss: 0.5474, Val Loss: 0.5511, Val ROC-AUC: 0.8521\n",
      "Epoch: 190, Loss: 0.5464, Val Loss: 0.5489, Val ROC-AUC: 0.8505\n",
      "Epoch: 200, Loss: 0.5514, Val Loss: 0.5501, Val ROC-AUC: 0.8515\n",
      "✓ Requirement 3.3: Model Training Complete\n",
      "\n",
      "--- Starting Visualization Generation ---\n",
      "✓ Requirement 4.1: Training Metrics Visualization Complete\n",
      "\n",
      "--- Starting Final Model Evaluation ---\n",
      "✓ Requirement 4.2: Model Evaluation Complete\n",
      "Final test metrics: {'roc_auc': 0.8770253942550819, 'average_precision': 0.8707082164189247, 'confusion_matrix': array([[ 9855,  6915],\n",
      "       [ 1010, 15760]])}\n",
      "\n",
      "--- Starting Genre Analysis ---\n",
      "✓ Requirement 4.3: Genre Analysis Complete\n",
      "\n",
      "Detailed Performance Metrics:\n",
      "ROC-AUC Score: 0.8770\n",
      "Average Precision: 0.8707\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9855  6915]\n",
      " [ 1010 15760]]\n",
      "\n",
      "Genre-wise Performance:\n",
      "classical_orchestral - ROC-AUC: 0.3542, F1: 0.7097\n",
      "electronic      - ROC-AUC: 0.9929, F1: 0.8362\n",
      "jazz            - ROC-AUC: 1.0000, F1: 0.8889\n",
      "hip_hop         - ROC-AUC: 0.9519, F1: 0.8288\n",
      "latin           - ROC-AUC: 0.8082, F1: 0.8114\n",
      "metal           - ROC-AUC: 1.0000, F1: 0.6667\n",
      "pop             - ROC-AUC: 0.9566, F1: 0.8207\n",
      "randb_Soul      - ROC-AUC: 0.9969, F1: 0.8435\n",
      "reggae_dancehall - ROC-AUC: 0.9249, F1: 0.8434\n",
      "rock            - ROC-AUC: 0.9068, F1: 0.8254\n",
      "soundtrack      - ROC-AUC: 0.5156, F1: 0.8000\n",
      "\n",
      "--- Starting Collaboration Prediction ---\n",
      "\n",
      "Top predicted collaborations:\n",
      "Artist 1: BASI, Artist 2: Abija, Score: 0.7185\n",
      "Artist 1: PTAF, Artist 2: gnash, Score: 0.6939\n",
      "Artist 1: Mabel, Artist 2: Ash, Score: 0.6889\n",
      "Artist 1: Luca Dayz, Artist 2: Aufgang, Score: 0.6787\n",
      "Artist 1: LUCK MUZIK, Artist 2: Tyla, Score: 0.6751\n",
      "Artist 1: Luke Burr, Artist 2: NCT 2018, Score: 0.6723\n",
      "Artist 1: FOOL, Artist 2: Eats Everything, Score: 0.6677\n",
      "Artist 1: Tommy Cash, Artist 2: Raluka, Score: 0.6675\n",
      "Artist 1: Disko Warp, Artist 2: Tom & Hills, Score: 0.6625\n",
      "Artist 1: Mystique, Artist 2: Arda Gezer, Score: 0.6572\n",
      "\n",
      "Training complete. Check the following files for visualizations:\n",
      "1. training_metrics.png - Training progress visualization\n",
      "2. genre_distribution.png - Distribution of genres in the dataset\n",
      "3. genre_performance.png - Model performance across different genres\n",
      "4. confusion_matrix.png - Confusion matrix for test predictions\n",
      "5. precision_recall_curve.png - Precision-Recall curve\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Enhanced main pipeline with additional features.\"\"\"\n",
    "    print(\"\\n=== Starting Project Requirements Tracking ===\")\n",
    "    # Set up device first\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"\\n✓ Requirement 1.1: Device Setup Complete - Using {device}\")\n",
    "\n",
    "    # Load and validate data\n",
    "    print(\"\\n--- Starting Data Loading and Validation ---\")\n",
    "    nodes_df, edge_index, node_features, node_index_map, genre_columns = load_data(validate=True)\n",
    "    print(\"✓ Requirement 1.2: Data Loading and Validation Complete\")\n",
    "    print(f\"  • Loaded {len(nodes_df)} artists\")\n",
    "    print(f\"  • Processed {edge_index.size(1)} collaborations\")\n",
    "    print(f\"  • Extracted {len(genre_columns)} genre features\")\n",
    "\n",
    "    # Move data to device immediately after creation\n",
    "    data = Data(\n",
    "        x=node_features.to(device),\n",
    "        edge_index=edge_index.to(device)\n",
    "    )\n",
    "    print(\"✓ Requirement 1.3: Data Movement to Device Complete\")\n",
    "\n",
    "    # Split data\n",
    "    print(\"\\n--- Starting Data Splitting ---\")\n",
    "    num_edges = edge_index.size(1)\n",
    "    all_edges = edge_index.t().contiguous()\n",
    "\n",
    "    train_edges, temp_edges = train_test_split(all_edges, test_size=0.3, random_state=42)\n",
    "    val_edges, test_edges = train_test_split(temp_edges, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Move all edges to device\n",
    "    train_edges = train_edges.to(device)\n",
    "    val_edges = val_edges.to(device)\n",
    "    test_edges = test_edges.to(device)\n",
    "    print(\"✓ Requirement 2.1: Data Splitting Complete\")\n",
    "    print(f\"  • Training edges: {len(train_edges)}\")\n",
    "    print(f\"  • Validation edges: {len(val_edges)}\")\n",
    "    print(f\"  • Test edges: {len(test_edges)}\")\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    print(\"\\n--- Starting Hyperparameter Tuning ---\")\n",
    "    param_grid = {\n",
    "        'in_channels': [data.num_node_features],\n",
    "        'hidden_channels': [32, 64, 128],\n",
    "        'heads': [4, 8, 16]\n",
    "    }\n",
    "    best_params = HyperparameterTuner.grid_search(GAT, data, param_grid)\n",
    "    print(\"✓ Requirement 2.2: Hyperparameter Tuning Complete\")\n",
    "    print(f\"  • Best parameters found: {best_params}\")\n",
    "    logging.info(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "    # Initialize model with best parameters\n",
    "    print(\"\\n--- Starting Model Training ---\")\n",
    "    model = GAT(\n",
    "        in_channels=data.num_node_features,\n",
    "        hidden_channels=best_params['hidden_channels'],\n",
    "        heads=best_params['heads']\n",
    "    ).to(device)\n",
    "    print(\"✓ Requirement 3.1: Model Initialization Complete\")\n",
    "\n",
    "    # Rest of the training pipeline...\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    print(\"✓ Requirement 3.2: Optimizer and Scheduler Setup Complete\")\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    print(\"\\n--- Starting Training Loop ---\")\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "    roc_auc_history = []\n",
    "\n",
    "    best_val_score = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss, val_loss = train_with_validation(model, data, train_edges, val_edges, optimizer, scheduler)\n",
    "        loss_history.append(loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            val_metrics = evaluate_model_performance(model, data, val_edges)\n",
    "            val_score = val_metrics['roc_auc']\n",
    "            roc_auc_history.append(val_score)\n",
    "\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                save_model(model, 'best_model_final.pth')\n",
    "\n",
    "            logging.info(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val ROC-AUC: {val_score:.4f}\")\n",
    "            print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val ROC-AUC: {val_score:.4f}\")\n",
    "\n",
    "    print(\"✓ Requirement 3.3: Model Training Complete\")\n",
    "\n",
    "    # Plot training metrics\n",
    "    print(\"\\n--- Starting Visualization Generation ---\")\n",
    "    plot_training_metrics(loss_history, val_loss_history, roc_auc_history)\n",
    "    print(\"✓ Requirement 4.1: Training Metrics Visualization Complete\")\n",
    "\n",
    "    # Load best model and evaluate\n",
    "    print(\"\\n--- Starting Final Model Evaluation ---\")\n",
    "    model = load_model(model, 'best_model_final.pth', device)\n",
    "    test_metrics = evaluate_model_performance(model, data, test_edges)\n",
    "    print(\"✓ Requirement 4.2: Model Evaluation Complete\")\n",
    "    logging.info(f\"Final test metrics: {test_metrics}\")\n",
    "    print(f\"Final test metrics: {test_metrics}\")\n",
    "\n",
    "    # Genre-specific performance analysis\n",
    "    print(\"\\n--- Starting Genre Analysis ---\")\n",
    "    genre_performance = analyze_genre_specific_performance(model, data, test_edges, nodes_df, genre_columns)\n",
    "    logging.info(f\"Genre-specific performance: {genre_performance}\")\n",
    "\n",
    "    # Plot genre performance\n",
    "    plot_genre_performance(genre_performance)\n",
    "    print(\"✓ Requirement 4.3: Genre Analysis Complete\")\n",
    "\n",
    "    print(\"\\nDetailed Performance Metrics:\")\n",
    "    print(f\"ROC-AUC Score: {test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"Average Precision: {test_metrics['average_precision']:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(test_metrics['confusion_matrix'])\n",
    "\n",
    "    # Add genre performance summary\n",
    "    print(\"\\nGenre-wise Performance:\")\n",
    "    for genre, metrics in genre_performance.items():\n",
    "        print(f\"{genre:15} - ROC-AUC: {metrics['roc_auc']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "    # Predict new collaborations\n",
    "    print(\"\\n--- Starting Collaboration Prediction ---\")\n",
    "    print(\"\\nTop predicted collaborations:\")\n",
    "    predictions = predict_new_collaborations(model, data, nodes_df, node_index_map)\n",
    "    for pred in predictions:\n",
    "        print(f\"Artist 1: {pred['artist1']}, Artist 2: {pred['artist2']}, Score: {pred['score']:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete. Check the following files for visualizations:\")\n",
    "    print(\"1. training_metrics.png - Training progress visualization\")\n",
    "    print(\"2. genre_distribution.png - Distribution of genres in the dataset\")\n",
    "    print(\"3. genre_performance.png - Model performance across different genres\")\n",
    "    print(\"4. confusion_matrix.png - Confusion matrix for test predictions\")\n",
    "    print(\"5. precision_recall_curve.png - Precision-Recall curve\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWk6ZJG9vuRn"
   },
   "source": [
    "# Main function with tsne visualization(optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "XVasOHuCuwrr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Project Requirements Tracking ===\n",
      "Using device: cpu\n",
      "\n",
      "--- Loading and Validating Data ---\n",
      "  • Validating data structure...\n",
      "Total artists: 120353\n",
      "Artists with genre info: 25576\n",
      "\n",
      "--- Visualizing Graph Degree Distribution ---\n",
      "✓ Degree Distribution Plot Saved.\n",
      "\n",
      "--- Splitting Data ---\n",
      "\n",
      "--- Initializing Model ---\n",
      "\n",
      "--- Generating Initial Embedding Visualization ---\n",
      "✓ Initial t-SNE and UMAP Visualizations Saved.\n",
      "\n",
      "--- Training Model ---\n",
      "Epoch: 010, Loss: 0.5923, Val Loss: 0.5426, Val ROC-AUC: 0.8692\n",
      "Epoch: 020, Loss: 0.5741, Val Loss: 0.5408, Val ROC-AUC: 0.8809\n",
      "Epoch: 030, Loss: 0.5643, Val Loss: 0.5516, Val ROC-AUC: 0.8684\n",
      "Epoch: 040, Loss: 0.5568, Val Loss: 0.5405, Val ROC-AUC: 0.8669\n",
      "Epoch: 050, Loss: 0.5635, Val Loss: 0.5422, Val ROC-AUC: 0.8653\n",
      "Epoch: 060, Loss: 0.5589, Val Loss: 0.5438, Val ROC-AUC: 0.8622\n",
      "Epoch: 070, Loss: 0.5581, Val Loss: 0.5436, Val ROC-AUC: 0.8613\n",
      "Epoch: 080, Loss: 0.5632, Val Loss: 0.5431, Val ROC-AUC: 0.8654\n",
      "Epoch: 090, Loss: 0.5628, Val Loss: 0.5432, Val ROC-AUC: 0.8648\n",
      "Epoch: 100, Loss: 0.5636, Val Loss: 0.5424, Val ROC-AUC: 0.8637\n",
      "Epoch: 110, Loss: 0.5579, Val Loss: 0.5437, Val ROC-AUC: 0.8626\n",
      "Epoch: 120, Loss: 0.5652, Val Loss: 0.5428, Val ROC-AUC: 0.8628\n",
      "Epoch: 130, Loss: 0.5617, Val Loss: 0.5420, Val ROC-AUC: 0.8631\n",
      "Epoch: 140, Loss: 0.5644, Val Loss: 0.5443, Val ROC-AUC: 0.8652\n",
      "Epoch: 150, Loss: 0.5633, Val Loss: 0.5431, Val ROC-AUC: 0.8652\n",
      "Epoch: 160, Loss: 0.5598, Val Loss: 0.5426, Val ROC-AUC: 0.8648\n",
      "Epoch: 170, Loss: 0.5589, Val Loss: 0.5450, Val ROC-AUC: 0.8633\n",
      "Epoch: 180, Loss: 0.5637, Val Loss: 0.5446, Val ROC-AUC: 0.8649\n",
      "Epoch: 190, Loss: 0.5587, Val Loss: 0.5433, Val ROC-AUC: 0.8643\n",
      "Epoch: 200, Loss: 0.5627, Val Loss: 0.5437, Val ROC-AUC: 0.8632\n",
      "✓ Training Completed.\n",
      "\n",
      "--- Generating Training Metrics Visualization ---\n",
      "✓ Training Metrics Visualization Saved.\n",
      "\n",
      "--- Evaluating Final Model ---\n",
      "✓ Model Evaluation Completed.\n",
      "\n",
      "--- Generating Final Embedding Visualization ---\n",
      "✓ Final t-SNE and UMAP Visualizations Saved.\n",
      "\n",
      "--- Analyzing Genre-Specific Performance ---\n",
      "✓ Genre Analysis Completed.\n",
      "\n",
      "--- Predicting New Collaborations ---\n",
      "Artist 1: Dargen D'Amico, Artist 2: El David Aguilar, Score: 0.6810\n",
      "Artist 1: Sir Mix-A-Lot, Artist 2: Yasumo, Score: 0.6774\n",
      "Artist 1: Goldie Loc, Artist 2: Inmo Yang, Score: 0.6610\n",
      "Artist 1: Kool Savas, Artist 2: Millonario, Score: 0.6369\n",
      "Artist 1: Badda TD, Artist 2: Dutch Nazari, Score: 0.6359\n",
      "Artist 1: Vittorio Grasso, Artist 2: dj newtown, Score: 0.6355\n",
      "Artist 1: blckfriend, Artist 2: Pete Philly, Score: 0.6252\n",
      "Artist 1: Samo, Artist 2: 13 Killoki, Score: 0.6153\n",
      "Artist 1: Yuridope, Artist 2: Spinache, Score: 0.6144\n",
      "Artist 1: Anna Frost, Artist 2: Lord Digga, Score: 0.6137\n",
      "\n",
      "Training complete. Check the following files for visualizations:\n",
      "1. training_metrics.png - Training progress visualization\n",
      "2. degree_distribution.png - Node degree distribution\n",
      "3. genre_distribution.png - Distribution of genres in the dataset\n",
      "4. genre_performance.png - Model performance across different genres\n",
      "5. confusion_matrix.png - Confusion matrix for test predictions\n",
      "6. precision_recall_curve.png - Precision-Recall curve\n",
      "7. tsne_initial.png - Initial t-SNE of node embeddings\n",
      "8. tsne_trained.png - Final t-SNE of node embeddings\n",
      "9. umap_initial.png - Initial UMAP of node embeddings\n",
      "10. umap_trained.png - Final UMAP of node embeddings\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Enhanced main pipeline with additional visualizations.\"\"\"\n",
    "    print(\"\\n=== Starting Project Requirements Tracking ===\")\n",
    "\n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load and validate data\n",
    "    print(\"\\n--- Loading and Validating Data ---\")\n",
    "    nodes_df, edge_index, node_features, node_index_map, genre_columns = load_data(validate=True)\n",
    "\n",
    "    # Move data to device\n",
    "    data = Data(\n",
    "        x=node_features.to(device),\n",
    "        edge_index=edge_index.to(device)\n",
    "    )\n",
    "\n",
    "    # Plot Degree Distribution\n",
    "    print(\"\\n--- Visualizing Graph Degree Distribution ---\")\n",
    "    plot_degree_distribution(edge_index, filename=\"degree_distribution.png\")\n",
    "    print(\"✓ Degree Distribution Plot Saved.\")\n",
    "\n",
    "    # Split Data (Train, Validation, Test)\n",
    "    print(\"\\n--- Splitting Data ---\")\n",
    "    num_edges = edge_index.size(1)\n",
    "    all_edges = edge_index.t().contiguous()\n",
    "\n",
    "    train_edges, temp_edges = train_test_split(all_edges, test_size=0.3, random_state=42)\n",
    "    val_edges, test_edges = train_test_split(temp_edges, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Move edges to device\n",
    "    train_edges = train_edges.to(device)\n",
    "    val_edges = val_edges.to(device)\n",
    "    test_edges = test_edges.to(device)\n",
    "\n",
    "    # Initialize Model\n",
    "    print(\"\\n--- Initializing Model ---\")\n",
    "    model = GAT(\n",
    "        in_channels=data.num_node_features,\n",
    "        hidden_channels=64,\n",
    "        heads=8\n",
    "    ).to(device)\n",
    "\n",
    "    # Store Initial Embeddings for Visualization\n",
    "    print(\"\\n--- Generating Initial Embedding Visualization ---\")\n",
    "    with torch.no_grad():\n",
    "        initial_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "    visualize_tsne_embeddings(initial_embeddings, nodes_df, genre_columns, filename=\"tsne_initial.png\")\n",
    "    visualize_umap_embeddings(initial_embeddings, nodes_df, genre_columns, filename=\"umap_initial.png\")\n",
    "    print(\"✓ Initial t-SNE and UMAP Visualizations Saved.\")\n",
    "\n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    # Train Model\n",
    "    print(\"\\n--- Training Model ---\")\n",
    "    loss_history = []\n",
    "    val_loss_history = []\n",
    "    roc_auc_history = []\n",
    "\n",
    "    best_val_score = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss, val_loss = train_with_validation(model, data, train_edges, val_edges, optimizer, scheduler)\n",
    "        loss_history.append(loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            val_metrics = evaluate_model_performance(model, data, val_edges)\n",
    "            val_score = val_metrics['roc_auc']\n",
    "            roc_auc_history.append(val_score)\n",
    "\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                save_model(model, 'best_model_final.pth')\n",
    "\n",
    "            print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val ROC-AUC: {val_score:.4f}\")\n",
    "\n",
    "    print(\"✓ Training Completed.\")\n",
    "\n",
    "    # Plot Training Metrics\n",
    "    print(\"\\n--- Generating Training Metrics Visualization ---\")\n",
    "    plot_training_metrics(loss_history, val_loss_history, roc_auc_history)\n",
    "    print(\"✓ Training Metrics Visualization Saved.\")\n",
    "\n",
    "    # Load Best Model & Evaluate\n",
    "    print(\"\\n--- Evaluating Final Model ---\")\n",
    "    model = load_model(model, 'best_model_final.pth', device)\n",
    "    test_metrics = evaluate_model_performance(model, data, test_edges)\n",
    "    print(\"✓ Model Evaluation Completed.\")\n",
    "\n",
    "    # Generate Final Embeddings & Visualizations\n",
    "    print(\"\\n--- Generating Final Embedding Visualization ---\")\n",
    "    with torch.no_grad():\n",
    "        trained_embeddings = model(data.x, data.edge_index)\n",
    "\n",
    "    visualize_tsne_embeddings(trained_embeddings, nodes_df, genre_columns, filename=\"tsne_trained.png\")\n",
    "    visualize_umap_embeddings(trained_embeddings, nodes_df, genre_columns, filename=\"umap_trained.png\")\n",
    "    print(\"✓ Final t-SNE and UMAP Visualizations Saved.\")\n",
    "\n",
    "    # Analyze Genre-Specific Performance\n",
    "    print(\"\\n--- Analyzing Genre-Specific Performance ---\")\n",
    "    genre_performance = analyze_genre_specific_performance(model, data, test_edges, nodes_df, genre_columns)\n",
    "    plot_genre_performance(genre_performance)\n",
    "    print(\"✓ Genre Analysis Completed.\")\n",
    "\n",
    "    # Predict New Collaborations\n",
    "    print(\"\\n--- Predicting New Collaborations ---\")\n",
    "    predictions = predict_new_collaborations(model, data, nodes_df, node_index_map)\n",
    "\n",
    "    for pred in predictions[:10]:  # Show only top 10\n",
    "        print(f\"Artist 1: {pred['artist1']}, Artist 2: {pred['artist2']}, Score: {pred['score']:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete. Check the following files for visualizations:\")\n",
    "    print(\"1. training_metrics.png - Training progress visualization\")\n",
    "    print(\"2. degree_distribution.png - Node degree distribution\")\n",
    "    print(\"3. genre_distribution.png - Distribution of genres in the dataset\")\n",
    "    print(\"4. genre_performance.png - Model performance across different genres\")\n",
    "    print(\"5. confusion_matrix.png - Confusion matrix for test predictions\")\n",
    "    print(\"6. precision_recall_curve.png - Precision-Recall curve\")\n",
    "    print(\"7. tsne_initial.png - Initial t-SNE of node embeddings\")\n",
    "    print(\"8. tsne_trained.png - Final t-SNE of node embeddings\")\n",
    "    print(\"9. umap_initial.png - Initial UMAP of node embeddings\")\n",
    "    print(\"10. umap_trained.png - Final UMAP of node embeddings\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
