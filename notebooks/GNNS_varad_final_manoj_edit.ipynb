{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435fada9-1a5d-4006-a9e4-9549b042e722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13265/4278976696.py:17: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nodes_df = pd.read_csv(\"nodes_cleaned.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading cleaned nodes and edges data\n",
    "nodes_df = pd.read_csv(\"nodes_cleaned.csv\")  \n",
    "edges_df = pd.read_csv(\"edges_cleaned.csv\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d76357-0b48-4da7-a33a-b2db78bb5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nodes_df['spotify_id'].duplicated().any():\n",
    "    nodes_df = nodes_df.drop_duplicates(subset=['spotify_id']).reset_index(drop=True)\n",
    "if nodes_df['spotify_id'].duplicated().any():\n",
    "    raise ValueError(\"Duplicate Spotify IDs found in nodes_df.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15389b53-34de-4725-aa16-ccec902c4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping Spotify IDs to indices for graph construction\n",
    "node_index_map = {spotify_id: idx for idx, spotify_id in enumerate(nodes_df['spotify_id'])}\n",
    "edges_df['Source'] = edges_df['id_0'].map(node_index_map)\n",
    "edges_df['Target'] = edges_df['id_1'].map(node_index_map)\n",
    "\n",
    "\n",
    "# Creating edge index (two rows: source and target nodes)\n",
    "edge_index = torch.tensor(edges_df[['Source', 'Target']].to_numpy().T, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17dd2a50-6656-4d16-8392-2eb45d028563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to generate negative edges\n",
    "def generate_negative_edges(num_nodes, existing_edges, num_samples):\n",
    "    \"\"\"\n",
    "    Generate negative edges by sampling random node pairs that are not connected.\n",
    "    \"\"\"\n",
    "    # Convert to set for fast lookup\n",
    "    existing_edges_set = set(map(tuple, existing_edges.T.tolist()))  \n",
    "    negative_edges = set()\n",
    "\n",
    "    while len(negative_edges) < num_samples:\n",
    "        i, j = np.random.randint(0, num_nodes, size=2)\n",
    "        if i != j and (i, j) not in existing_edges_set and (j, i) not in existing_edges_set:\n",
    "            negative_edges.add((i, j))\n",
    "\n",
    "    return torch.tensor(list(negative_edges), dtype=torch.long)\n",
    "\n",
    "# Generating negative edges\n",
    "num_negative_samples = len(edges_df)  # Same as the number of positive edges\n",
    "negative_edges = generate_negative_edges(len(nodes_df), edge_index, num_negative_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd00ee74-2821-42b4-ac3f-048664951e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b531a0a07d784719ae770596a46ae3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/135058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 7/7 [01:52<00:00, 16.10s/it]\n",
      "Generating walks (CPU: 3): 100%|██████████| 6/6 [01:57<00:00, 19.54s/it]\n",
      "Generating walks (CPU: 2): 100%|██████████| 7/7 [02:11<00:00, 18.79s/it]\n",
      "Generating walks (CPU: 4): 100%|██████████| 6/6 [02:01<00:00, 20.30s/it]\n",
      "Generating walks (CPU: 5): 100%|██████████| 6/6 [02:03<00:00, 20.56s/it]\n",
      "Generating walks (CPU: 6): 100%|██████████| 6/6 [01:48<00:00, 18.05s/it]\n",
      "Generating walks (CPU: 7): 100%|██████████| 6/6 [01:37<00:00, 16.23s/it]\n",
      "Generating walks (CPU: 8): 100%|██████████| 6/6 [01:17<00:00, 12.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initializing and fit Node2Vec\u001b[39;00m\n\u001b[1;32m      9\u001b[0m node2vec \u001b[38;5;241m=\u001b[39m Node2Vec(graph, dimensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, walk_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_walks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnode2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Geting node embeddings, handling missing nodes\u001b[39;00m\n\u001b[1;32m     13\u001b[0m node_embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages/node2vec/node2vec.py:198\u001b[0m, in \u001b[0;36mNode2Vec.fit\u001b[0;34m(self, **skip_gram_params)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip_gram_params:\n\u001b[1;32m    196\u001b[0m     skip_gram_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mskip_gram_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[1;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m~/tuhh/data_science_3rd_sem/intelligent_system_in_medicine/ism_project_git/venv/lib/python3.10/site-packages/gensim/models/word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Combining positive and negative edges for training\n",
    "positive_edges = edge_index.T\n",
    "\n",
    "# Convert edge_index to a NetworkX graph\n",
    "graph = nx.Graph()\n",
    "graph.add_edges_from(edge_index.T.tolist())\n",
    "\n",
    "# Initializing and fit Node2Vec\n",
    "node2vec = Node2Vec(graph, dimensions=32, walk_length=10, num_walks=50, workers=8, p=1, q=2)\n",
    "model = node2vec.fit(window=5, min_count=1, batch_words=4)\n",
    "\n",
    "# Geting node embeddings, handling missing nodes\n",
    "node_embeddings = []\n",
    "for node in range(len(nodes_df)):\n",
    "    if str(node) in model.wv:  # Check if the node exists in the Node2Vec model\n",
    "        node_embeddings.append(model.wv[str(node)])  # Use the embedding from the model\n",
    "    else:\n",
    "        node_embeddings.append(np.zeros(model.wv.vector_size))  # Initialize missing embeddings with zeros\n",
    "\n",
    "# Converting the embeddings list to a NumPy array\n",
    "node_embeddings = np.array(node_embeddings)\n",
    "\n",
    "# Saving the embeddings to a file\n",
    "np.save('node_embeddings.npy', node_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bc5ae0-a2ed-4931-8b19-19448e9f33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_keys = list(graph.nodes)\n",
    "print(len(node_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b10657-42d6-4259-9cfc-24ceddeb790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.tensor(node_embeddings, dtype=torch.float)  # Update node features\n",
    "\n",
    "all_edges = torch.cat([positive_edges, negative_edges], dim=0)\n",
    "labels = torch.cat([torch.ones(len(positive_edges)), torch.zeros(len(negative_edges))])\n",
    "\n",
    "# Train-test split for edges\n",
    "train_edges, test_edges, train_labels, test_labels = train_test_split(\n",
    "    all_edges, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Converting to PyTorch Geometric Data\n",
    "data = Data(x=node_features, edge_index=edge_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a707a-11b6-4d9d-a8d1-c039ed345380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Seting a low perplexity (to make sure it's less than the number of nodes in your subset)\n",
    "perplexity_value = 5\n",
    "\n",
    "# Reducing embeddings to 2D using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "\n",
    "# Selecting a subset of nodes to visualize (example: top 20 based on popularity/followers or specific genres)\n",
    "subset_size = 20\n",
    "subset_indices = nodes_df.nlargest(subset_size, 'followers').index  \n",
    "\n",
    "# Geting corresponding embeddings for the subset\n",
    "subset_embeddings = node_embeddings[subset_indices]\n",
    "\n",
    "# Applying t-SNE to the subset of embeddings\n",
    "reduced_embeddings = tsne.fit_transform(subset_embeddings)\n",
    "\n",
    "# Creating a scatter plot for the subset\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1], s=100, color=\"blue\")\n",
    "\n",
    "# adding labels to the points (nodes)\n",
    "for i, idx in enumerate(subset_indices):\n",
    "    plt.text(reduced_embeddings[i, 0], reduced_embeddings[i, 1], nodes_df.loc[idx, 'name'], fontsize=12)\n",
    "\n",
    "plt.title(f\"2D Visualization of Top {subset_size} Node Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c8233-df86-427a-92f4-6ac5b25d214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining GraphSAGE Model for Link Prediction\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.edge_predictor = torch.nn.Linear(hidden_dim * 2, 1)  # Combining two embeddings\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.conv1(data.x, data.edge_index))\n",
    "        x = F.relu(self.conv2(x, data.edge_index))\n",
    "        return x\n",
    "\n",
    "    def predict_edges(self, x, edges):\n",
    "        # Combine node embeddings for edge prediction\n",
    "        edge_embeds = torch.cat([x[edges[:, 0]], x[edges[:, 1]]], dim=1)\n",
    "        return torch.sigmoid(self.edge_predictor(edge_embeds)).squeeze()\n",
    "\n",
    "# Initializing model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGE(data.num_node_features, hidden_dim=32).to(device)\n",
    "data = data.to(device)\n",
    "train_edges, train_labels = train_edges.to(device), train_labels.to(device)\n",
    "test_edges, test_labels = test_edges.to(device), test_labels.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18c7a9-0401-467c-8475-62edd4b0ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    embeddings = model(data)\n",
    "    pred = model.predict_edges(embeddings, train_edges)\n",
    "    loss = F.binary_cross_entropy(pred, train_labels.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(data)\n",
    "        pred = model.predict_edges(embeddings, test_edges)\n",
    "        pred_labels = (pred > 0.5).float()  # Binary predictions\n",
    "        accuracy = (pred_labels == test_labels.float()).sum() / len(test_labels)\n",
    "\n",
    "        # Return accuracy, true labels, and predicted labels\n",
    "        return accuracy.item(), test_labels.cpu().numpy(), pred_labels.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e6694-8d43-4069-96ec-2ec124a77dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    if epoch % 10 == 0:\n",
    "        acc, y_true, y_pred = test()\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Test Accuracy: {acc:.4f}')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()  # Extract TN, FP, FN, TP\n",
    "\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"True Negatives (TN): {tn}\")\n",
    "        print(f\"False Positives (FP): {fp}\")\n",
    "        print(f\"False Negatives (FN): {fn}\")\n",
    "        print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(f\"TN: Predicted no collaboration correctly (actual no collaboration)\")\n",
    "        print(f\"FP: Predicted collaboration incorrectly (actual no collaboration)\")\n",
    "        print(f\"FN: Predicted no collaboration incorrectly (actual collaboration)\")\n",
    "        print(f\"TP: Predicted collaboration correctly (actual collaboration)\")\n",
    "\n",
    "        # Optionally, print a classification report\n",
    "        report = classification_report(y_true, y_pred, target_names=[\"No Collaboration\", \"Collaboration\"])\n",
    "        print(f'\\nClassification Report:\\n{report}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f14806-847b-463f-bdd5-acd60d28f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Converting existing edges to a set for fast lookup\n",
    "existing_edges_set = set(map(tuple, edge_index.T.tolist()))  # Set of existing edges for quick lookup\n",
    "\n",
    "# Number of unseen pairs to sample\n",
    "num_samples = 300\n",
    "unseen_pairs = []\n",
    "\n",
    "# Randomly sampling node pairs and check if they're not connected\n",
    "while len(unseen_pairs) < num_samples:\n",
    "    i, j = random.sample(range(len(nodes_df)), 2)  # Randomly sample two nodes\n",
    "    \n",
    "    # Ensuring the pair is not already an existing edge\n",
    "    if i != j and (i, j) not in existing_edges_set and (j, i) not in existing_edges_set:\n",
    "        unseen_pairs.append((i, j))\n",
    "\n",
    "# Converting to tensor\n",
    "unseen_pairs = torch.tensor(unseen_pairs, dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639c188-520a-4355-8504-c84889c2bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Seting model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    embeddings = model(data)  # Geting node embeddings\n",
    "    predictions = model.predict_edges(embeddings, unseen_pairs)  # Predicting for unseen pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe936a-d2a8-44c3-ad87-7e0e49adc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking unseen pairs by predicted score\n",
    "sorted_indices = torch.argsort(predictions, descending=True)\n",
    "top_predictions = unseen_pairs[sorted_indices]\n",
    "top_scores = predictions[sorted_indices]\n",
    "\n",
    "# Converting back to artist IDs for better interpretability\n",
    "top_collaborations = [\n",
    "    (nodes_df.iloc[i]['spotify_id'], nodes_df.iloc[j]['spotify_id'], score.item())\n",
    "    for (i, j), score in zip(top_predictions.tolist(), top_scores)\n",
    "]\n",
    "\n",
    "# Displaying top 10 predicted collaborations with artist names\n",
    "\n",
    "for pair in top_collaborations[:10]:\n",
    "    spotify_id_1, spotify_id_2 = pair[0], pair[1]\n",
    "    \n",
    "    # Searching for the Spotify ID in the nodes_df and print the respective artist names\n",
    "    artist_1_name = nodes_df[nodes_df['spotify_id'] == spotify_id_1]['name'].values[0]\n",
    "    artist_2_name = nodes_df[nodes_df['spotify_id'] == spotify_id_2]['name'].values[0]\n",
    "    \n",
    "    print(f\"Artist 1: {artist_1_name}, Artist 2: {artist_2_name}, Predicted Score: {pair[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9008055-1afb-42e8-a66f-dad43176ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the top 10 predictions for visualization\n",
    "top_10_collaborations = top_collaborations[:10]\n",
    "\n",
    "# Prepare data for plotting\n",
    "artist_pairs = [f\"{nodes_df[nodes_df['spotify_id'] == pair[0]]['name'].values[0]} & {nodes_df[nodes_df['spotify_id'] == pair[1]]['name'].values[0]}\" for pair in top_10_collaborations]\n",
    "scores = [pair[2] for pair in top_10_collaborations]\n",
    "\n",
    "# Plotting the top 10 collaborations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(artist_pairs, scores, color='skyblue')\n",
    "plt.xlabel('Predicted Collaboration Score')\n",
    "plt.title('Top 10 Predicted Artist Collaborations')\n",
    "plt.gca().invert_yaxis()  # To show the highest scores on top\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
